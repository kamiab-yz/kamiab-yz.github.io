---
permalink: /
title: "Kamyab Yazdipaz"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<table style="width:100%; border:none; border-collapse:collapse;">
  <tr>
    <td style="width:55%; vertical-align:top; border:none; padding-right:20px; text-align: justify;">
      I'm a PhD student in Mechanical Engineering (Robotics) at <a href="https://www.wsu.edu">Washington State University</a>, conducting research in the <a href="https://labs.wsu.edu/siaslab/">SIAS Lab</a>, under the supervision of <a href="https://mme.wsu.edu/mme-personnel/wsu-profile/mehdi.hosseinzadeh/">Dr. Mehdi Hosseinzadeh</a>. 
      <br><br>
      My research lies at the  intersection of robot perception, computer vision, and human–robot interaction. My work focuses on enabling intelligent robotic systems to reason about human behavior, perceptual awareness, and intent through multimodal sensing and learning-based methods. I am particularly interested in integrating vision–language models, probabilistic reasoning, and motion planning to support robust mobile robot navigation and decision-making in complex, cluttered environments, enabling safe, adaptive, and socially-aware robot behavior in human-centered settings.
    <!-- </td>
    <td style="width:45%; vertical-align:top; border:none; text-align:center;">
      <img src="images/Hierarchical_Control_Structure.png" alt="Hierarchical Control Structure" style="width:100%; border-radius:10px;">
      <p style="font-size:0.8em; color:grey; margin-top:10px;"><i>Hierarchical Control Structure</i></p>
    </td> -->
  </tr>
</table>

---
<h2 style="border-bottom: 1px solid #ddd; padding-bottom: 10px;">Education</h2>
<table style="width:100%; border:none; border-collapse:collapse; margin-top:20px;">
  <tr>
    <td style="width:75%; vertical-align:top; border:none; padding-bottom:15px;">
      <strong>Washington State University (WSU)</strong> <br>
      PhD in Mechanical Engineering
    </td>
    <td style="width:25%; vertical-align:top; border:none; text-align:right; color:grey;">
      2025 – Present
    </td>
  </tr>

  <tr>
    <td style="width:75%; vertical-align:top; border:none; padding-bottom:15px;">
      <strong>Iran University of Science and Technology (IUST)</strong> <br>
      Master of Science in Mechanical Engineering 
    </td>
    <td style="width:25%; vertical-align:top; border:none; text-align:right; color:grey;">
      2020 – 2023
    </td>
  </tr>

  <tr>
    <td style="width:75%; vertical-align:top; border:none; padding-bottom:15px;">
      <strong>Amirkabir University of Technology (AUT)</strong> <br>
      Bachelor of Science in Mechanical Engineering
    </td>
    <td style="width:25%; vertical-align:top; border:none; text-align:right; color:grey;">
      2015 – 2020
    </td>
  </tr>
</table>

---

<h2 style="border-bottom: 1px solid #ddd; padding-bottom: 10px;">Research</h2>

<table style="width:100%; border:none; border-collapse:collapse;">
  <tr>
    <td style="width:50%; vertical-align:top; border:none; padding-right:20px; text-align: justify;">
      <h3><u>Safe Robot Action Planning under Human Behavioral Uncertainty with Probabilistic and Vision–Language Reasoning </u></h3>
      <br>
      <b>K, Yazdipaz</b>, M. Amiri, M. Hosseinzadeh
      <br>
      <i>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (Submitted)</i>, 2026
      <br>
      <a href="https://www.youtube.com/embed/g0OT-_c3K9Y?si=TnsMI1Sx9w_7QzYI">[video]</a>
      <br><br>
      The proposed action planner update the robot’s belief about human cooperation by fusing two latent attributes: Perceptual Awareness (inferred from head pose and gestures) and Navigational Responsiveness (inferred from trajectory adaptations). The framework dynamically weights these attributes based on the interaction phase to address behavioral uncertainty.
   
</td>
    <td style="width:50%; vertical-align:top; border:none; text-align:center;">
      <img src="images/HRI_video.gif" alt="Dynamic Embedding Optimization" style="width:100%; border-radius:10px;">
      <p style="font-size:0.8em; color:grey; margin-top:10px;"><i> Experimental data; CASE I: Aware/Responsive, CASE II: Unaware/unresponsive, CASE III: Aware/unresponsive,CASE IV: Initially Unaware/Responsive.</i></p>
    </td>
  </tr>
</table>


---
<table style="width:100%; border:none; border-collapse:collapse; margin-top:20px;">
  <tr>
    <td style="width:60%; vertical-align:top; border:none; padding-right:20px; text-align: justify;">
      <strong>Robust and Efficient Phase Estimation in Legged Robots via Signal Imaging and Deep Neural Networks</strong> 
      <br><br>
      A robust and real-time method for phase estimation in legged robots is proposed using signal imaging and lightweight deep neural networks. Time-series data from IMUs and joint encoders are transformed into informative phase images through techniques such as stacked channel imaging and recurrence plots, enabling accurate detection of stance and flight phases without relying on fragile force sensors.  Experimental validation on a robotic leg platform and independent datasets demonstrates high accuracy and strong generalization, highlighting the method’s potential for reliable locomotion control in complex and uncertain environments. 
      <br><br>
      <a href="[https://ieeexplore.ieee.org/document/YOUR_PAPER_ID](https://ieeexplore.ieee.org/abstract/document/10916661)" style="text-decoration:none; font-weight:bold; color:#a6192e;">[Paper Link]</a> 
    </td>
    <td style="width:40%; vertical-align:top; border:none; text-align:center;">
      <img src="images/phase_estimation.gif" alt="Phase Estimation Demo" style="width:100%; border-radius:10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
      <p style="font-size:0.8em; color:grey; margin-top:10px;"><i>Real-time Phase Estimation Visualization</i></p>
    </td>
  </tr>
</table>
